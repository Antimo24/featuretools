{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Featuretools provides users the ability to remove features that are unlikely to be useful in building an effective machine learning model. Reducing the number of features in the feature matrix can both produce better results in the model as well as reduce the computational cost involved in prediction.\n",
    "\n",
    "Featuretools enables users to perform feature selection on the results of Deep Feature Synthesis with three functions:\n",
    "\n",
    "- `ft.remove_highly_null_features`\n",
    "- `ft.remove_single_value_features`\n",
    "- `ft.remove_highly_correlated_features`\n",
    "\n",
    "Each of the functions takes in a calculated feature matrix and can optionally take in the list of Features as well. If just a feature matrix is used as input, then just a feature matrix will be returned with the offending columns removed, but if both the matrix and the Feature definitions are used, then the results of feature selection will, similarly, be the matrix and the Feature definitions.\n",
    "\n",
    "We will describe each of these functions in depth, but first we must create an entity set with which we can run `ft.dfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import featuretools as ft\n",
    "\n",
    "from featuretools.selection import (\n",
    "    remove_low_information_features,\n",
    "    remove_highly_correlated_features,\n",
    "    remove_highly_null_features,\n",
    "    remove_single_value_features,\n",
    ")\n",
    "\n",
    "from featuretools.primitives import NaturalLanguage\n",
    "\n",
    "\n",
    "df1 = pd.DataFrame({'id': [0, 1, 2, 3], \n",
    "                    'categories':['a','a','b','b'], \n",
    "                    'bools':[True, True, False, False],\n",
    "                    'half_nulls': [None, None, 88, 100],\n",
    "                    'quarter_nulls': [None, 1,1, 1],\n",
    "                    'all_nulls': pd.Series([None, None, None,None], dtype='float'),\n",
    "                    \"diff_ints\": [34, 11, 29, 91],\n",
    "                   \n",
    "                   })\n",
    "df2 = pd.DataFrame({\n",
    "    'id': [0, 1, 2, 3],\n",
    "    \"first_id\": [0, 1, 1, 3],\n",
    "    \"words\": [\"test\", \"this is a short sentence\", \"foo bar\", \"baz\"],\n",
    "    \"corr_words\": [4, 24, 7, 3],\n",
    "    'corr_1': [99, 100, 77, 33],\n",
    "    'corr_2': [99, 100, 77, 33],\n",
    "})\n",
    "\n",
    "entities = {\n",
    "        \"first\": (df1, 'id'),\n",
    "        \"second\": (df2, 'id',  None, {'words': NaturalLanguage}),\n",
    "    }\n",
    "\n",
    "es = ft.EntitySet(\"data\", entities, )\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Highly Null Features\n",
    "\n",
    "We might have a dataset with columns that have many null values, and, after Deep Feature Synthesis, it becomes apparent that many of the features created from those columns will also have many null values. In this case, we might want to remove any columns whose null values pass a certain threshold. Below is our feature matrix with such a case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fm, features = ft.dfs(entityset=es,\n",
    "                          target_entity=\"first\",\n",
    "                          trans_primitives=['add_numeric'],\n",
    "                          agg_primitives=[],\n",
    "                          max_depth=2)\n",
    "fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the above feature matrix and decide to remove the highly null features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.selection.remove_highly_null_features(fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that calling `remove_highly_null_features` didn't remove every column that contains a null. By default, we only remove columns where the percentage of null values is above 95%. If we want to lower that threshold, we can set the `pct_null_threshold` paramter ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_highly_null_features(fm, pct_null_threshold=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're left with a feature matrix containing mostly populated data!\n",
    "\n",
    "## Remove Single Value Features\n",
    "\n",
    "Another situation we might run into is one where our calculated features don't have any variance. In those cases, we are likely to want to remove the uninteresting columns. For that, we use `remove_single_value_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm, features = ft.dfs(entityset=es,\n",
    "                          target_entity=\"first\",\n",
    "                          trans_primitives=['is_null'],\n",
    "                          agg_primitives=[],\n",
    "                          max_depth=2)\n",
    "fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example of using `IsNull` as a primitive highlights a case where many columns all have the same value. Lets remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_single_value_features(fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we've actually lost two of the three columns with null values because, with the function used as it is above, null values are not considered in whether a column has only one unique value. If we'd like to consider `NaN` its own value, we can set `count_nan_as_value` to `True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_single_value_features(fm, count_nan_as_value=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Highly Correlated Features\n",
    "\n",
    "The last feature selection function we have allows us to remove columns that would likely be redundant to the model we're attempting to build by considering the correlation between pairs of calculated features.\n",
    "\n",
    "When two columns are determined to be highly correlated, we remove the more complex of the two. For example, say we have two features:\n",
    "\n",
    "- `col`\n",
    "- `-(col)`\n",
    "\n",
    "We can see that `-(col)` is just the negation of `col`, and so we can guess those columns are going to be highly correlated. `-(col)` has has the `Negate` primitive applied to it, so it is more complex than the identity feature `col`. Therefore, if we only want one of `col` and `-(col)`, we should keep the identity feature. For features that don't have an obvious difference in complexity, we discard the feature that comes later in the feature matrix. \n",
    "\n",
    "Let's try this out on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fm, features = ft.dfs(entityset=es,\n",
    "                          target_entity=\"second\",\n",
    "                          trans_primitives=['negate', 'num_characters'],\n",
    "                          agg_primitives=[],\n",
    "                          max_depth=2)\n",
    "fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have some pretty clear correlations here between all the columns and their negations, but we also have `corr_words` that matches the feature `NUM_CHARACTERS(words)`.\n",
    "\n",
    "Now, using `remove_highly_correlated_features`,our default threshold for correlation is 95% correlated, and we get all of the obviously correlated features removed, leaving just the less complex features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_highly_correlated_features(fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pct_corr_threshold\n",
    "\n",
    "We can lower the threshold at which to remove correlated features if we'd like to be more restrictive by using the `pct_corr_threshold` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_highly_correlated_features(fm,pct_corr_threshold=.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### features_to_check\n",
    "\n",
    "If we only want to check a subset of columns, we can set `features_to_check` to the list of columns whose correlation we'd like to check, and no columns outside of that list will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_highly_correlated_features(fm, features_to_check=['corr_1', 'corr_2', '-(corr_1)' ,'-(corr_2)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### features_to_keep\n",
    "\n",
    "To avoid having specific columns removed from the feature matrix, we can include a list of `features_to_keep`, and these features will not be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_highly_correlated_features(fm, features_to_keep=['-(corr_1)' ,'-(corr_2)'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
